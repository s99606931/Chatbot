# 서비스 배포 구성

## 1. AI 서비스 배포
### 1.1 LLM 서비스
```yaml
# k8s/ai/llm-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-service
spec:
  replicas: 2
  template:
    spec:
      containers:
      - name: llm
        image: erp-chatbot/llm:latest
        resources:
          limits:
            cpu: "10"
            memory: "32Gi"
            nvidia.com/gpu: "1"
          requests:
            cpu: "5"
            memory: "16Gi"
        env:
        - name: MODEL_PATH
          value: "/models/erp-llm"
        volumeMounts:
        - name: model-storage
          mountPath: /models
```

### 1.2 RAG 서비스
```yaml
# k8s/ai/rag-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rag-service
spec:
  replicas: 2
  template:
    spec:
      containers:
      - name: rag
        image: erp-chatbot/rag:latest
        resources:
          limits:
            cpu: "4"
            memory: "16Gi"
          requests:
            cpu: "2"
            memory: "8Gi"
```

## 2. 데이터베이스 배포
### 2.1 PostgreSQL
```yaml
# k8s/db/postgresql-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgresql
spec:
  replicas: 2
  template:
    spec:
      containers:
      - name: postgresql
        image: postgres:16
        resources:
          limits:
            cpu: "4"
            memory: "16Gi"
        env:
        - name: POSTGRES_DB
          value: "erp_chatbot"
        volumeMounts:
        - name: postgresql-data
          mountPath: /var/lib/postgresql/data
```

### 2.2 Redis 캐시
```yaml
# k8s/db/redis-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: redis
        image: redis:7.2
        resources:
          limits:
            cpu: "2"
            memory: "8Gi"
        args: ["--maxmemory", "6gb", "--maxmemory-policy", "allkeys-lru"]
```